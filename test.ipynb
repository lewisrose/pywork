{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'opt_param.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-24cc4fd9422f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Accuracy :\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m \u001b[0mexecuteConvolutionalNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-24cc4fd9422f>\u001b[0m in \u001b[0;36mexecuteConvolutionalNeuralNetwork\u001b[0;34m()\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;31m#\"\"\" Load parameters learned in the SparseAutoencoderLinear exercise \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mopt_param\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'opt_param.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m     \u001b[0mzca_white\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zca_white.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mmean_patch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean_patch.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'opt_param.npy'"
     ]
    }
   ],
   "source": [
    "# This piece of software is bound by The MIT License (MIT)\n",
    "# Copyright (c) 2014 Siddharth Agrawal\n",
    "# Code written by : Siddharth Agrawal\n",
    "# Email ID : siddharth.950@gmail.com\n",
    "\n",
    "import numpy\n",
    "import math\n",
    "import time\n",
    "import scipy.io\n",
    "import scipy.signal\n",
    "import scipy.optimize\n",
    "import matplotlib.pyplot\n",
    "\n",
    "###########################################################################################\n",
    "#\" The Convolutional Neural Network class \"\"\"\n",
    "\n",
    "class ConvolutionalNeuralNetwork(object):\n",
    "\n",
    "    #######################################################################################\n",
    "   # \"\"\" Initialization of the network \"\"\"\n",
    "\n",
    "    def __init__(self, W1, b1, zca_white, mean_patch, patch_dim, pool_dim):\n",
    "    \n",
    "        #\"\"\" Store the weights, taking into account preprocessing done \"\"\"\n",
    "    \n",
    "        self.W = numpy.dot(W1, zca_white)\n",
    "        self.b = b1 - numpy.dot(self.W, mean_patch)\n",
    "        \n",
    "       # \"\"\" Variables associated with the network \"\"\"\n",
    "        \n",
    "        self.patch_dim = patch_dim\n",
    "        self.pool_dim  = pool_dim\n",
    "\n",
    "    #######################################################################################\n",
    "   # \"\"\" Returns elementwise sigmoid output of input array \"\"\"\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "    \n",
    "        return (1 / (1 + numpy.exp(-x)))\n",
    "        \n",
    "    #######################################################################################\n",
    "   # \"\"\" Returns the convolved features of the input images \"\"\"\n",
    "    \n",
    "    def convolve(self, input_images, num_features):\n",
    "    \n",
    "       # \"\"\" Extract useful values \"\"\"\n",
    "    \n",
    "        image_dim      = input_images.shape[0]\n",
    "        image_channels = input_images.shape[2]\n",
    "        num_images     = input_images.shape[3]\n",
    "        \n",
    "        #\"\"\" Assign memory for the convolved features \"\"\"\n",
    "        \n",
    "        conv_dim           = image_dim - self.patch_dim + 1\n",
    "        convolved_features = numpy.zeros((num_features, num_images, conv_dim, conv_dim));\n",
    "        \n",
    "        for image_num in range(num_images):\n",
    "        \n",
    "            for feature_num in range(num_features):\n",
    "            \n",
    "                #\"\"\" Initialize convolved image as array of zeros \"\"\"\n",
    "            \n",
    "                convolved_image = numpy.zeros((conv_dim, conv_dim))\n",
    "                \n",
    "                for channel in range(image_channels):\n",
    "                \n",
    "                    #\"\"\" Extract feature corresponding to the indices \"\"\"\n",
    "                \n",
    "                    limit0  = self.patch_dim * self.patch_dim * channel\n",
    "                    limit1  = limit0 + self.patch_dim * self.patch_dim\n",
    "                    feature = self.W[feature_num, limit0 : limit1].reshape(self.patch_dim, self.patch_dim)\n",
    "                    \n",
    "                   # \"\"\" Image to be convolved \"\"\"\n",
    "                    \n",
    "                    image = input_images[:, :, channel, image_num]\n",
    "                    \n",
    "                    #\"\"\" Convolve image with the feature and add to existing matrix \"\"\"\n",
    "\n",
    "                    convolved_image = convolved_image + scipy.signal.convolve2d(image, feature, 'valid');\n",
    "                \n",
    "                #\"\"\" Take sigmoid transform and store \"\"\"\n",
    "                    \n",
    "                convolved_image = self.sigmoid(convolved_image + self.b[feature_num, 0])\n",
    "                convolved_features[feature_num, image_num, :, :] = convolved_image\n",
    "                \n",
    "        return convolved_features\n",
    "        \n",
    "    #######################################################################################\n",
    "   # \"\"\" Pools the given convolved features \"\"\"\n",
    "    \n",
    "    def pool(self, convolved_features):\n",
    "    \n",
    "  #     \"\"\" Extract useful values \"\"\"\n",
    "    \n",
    "        num_features = convolved_features.shape[0]\n",
    "        num_images   = convolved_features.shape[1]\n",
    "        conv_dim     = convolved_features.shape[2]\n",
    "        res_dim      = conv_dim / self.pool_dim\n",
    "        \n",
    "     #   \"\"\" Initialize pooled features as array of zeros \"\"\"\n",
    "        \n",
    "        pooled_features = numpy.zeros((num_features, num_images, res_dim, res_dim))\n",
    "        \n",
    "        for image_num in range(num_images):\n",
    "        \n",
    "            for feature_num in range(num_features):\n",
    "            \n",
    "                for pool_row in range(res_dim):\n",
    "                \n",
    "                    row_start = pool_row * self.pool_dim\n",
    "                    row_end   = row_start + self.pool_dim\n",
    "                    \n",
    "                    for pool_col in range(res_dim):\n",
    "                    \n",
    "                        col_start = pool_col * self.pool_dim\n",
    "                        col_end   = col_start + self.pool_dim\n",
    "                        \n",
    "                        \"\"\" Extract image patch and calculate mean pool \"\"\"\n",
    "                        \n",
    "                        patch = convolved_features[feature_num, image_num, row_start : row_end,\n",
    "                                                   col_start : col_end]\n",
    "                        pooled_features[feature_num, image_num, pool_row, pool_col] = numpy.mean(patch)\n",
    "                        \n",
    "        return pooled_features\n",
    "        \n",
    "###########################################################################################\n",
    "#\"\"\" The Softmax Regression class \"\"\"\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "\n",
    "    #######################################################################################\n",
    "   # \"\"\" Initialization of Regressor object \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, num_classes, lamda):\n",
    "    \n",
    "       # \"\"\" Initialize parameters of the Regressor object \"\"\"\n",
    "    \n",
    "        self.input_size  = input_size  # input vector size\n",
    "        self.num_classes = num_classes # number of classes\n",
    "        self.lamda       = lamda       # weight decay parameter\n",
    "        \n",
    "        #\"\"\" Randomly initialize the class weights \"\"\"\n",
    "        \n",
    "        rand = numpy.random.RandomState(int(time.time()))\n",
    "        \n",
    "        self.theta = 0.005 * numpy.asarray(rand.normal(size = (num_classes*input_size, 1)))\n",
    "    \n",
    "    #######################################################################################\n",
    "  #  \"\"\" Returns the groundtruth matrix for a set of labels \"\"\"\n",
    "        \n",
    "    def getGroundTruth(self, labels):\n",
    "    \n",
    "      #  \"\"\" Prepare data needed to construct groundtruth matrix \"\"\"\n",
    "    \n",
    "        labels = numpy.array(labels).flatten()\n",
    "        data   = numpy.ones(len(labels))\n",
    "        indptr = numpy.arange(len(labels)+1)\n",
    "        \n",
    "     #   \"\"\" Compute the groundtruth matrix and return \"\"\"\n",
    "        \n",
    "        ground_truth = scipy.sparse.csr_matrix((data, labels, indptr))\n",
    "        ground_truth = numpy.transpose(ground_truth.todense())\n",
    "        \n",
    "        return ground_truth\n",
    "        \n",
    "    #######################################################################################\n",
    "   # \"\"\" Returns the cost and gradient of 'theta' at a particular 'theta' \"\"\"\n",
    "        \n",
    "    def softmaxCost(self, theta, input, labels):\n",
    "    \n",
    "       # \"\"\" Compute the groundtruth matrix \"\"\"\n",
    "    \n",
    "        ground_truth = self.getGroundTruth(labels)\n",
    "        \n",
    "        #\"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "        \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "      #  \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "      #  \"\"\" Compute the traditional cost term \"\"\"\n",
    "        \n",
    "        cost_examples    = numpy.multiply(ground_truth, numpy.log(probabilities))\n",
    "        traditional_cost = -(numpy.sum(cost_examples) / input.shape[1])\n",
    "        \n",
    "      #  \"\"\" Compute the weight decay term \"\"\"\n",
    "        \n",
    "        theta_squared = numpy.multiply(theta, theta)\n",
    "        weight_decay  = 0.5 * self.lamda * numpy.sum(theta_squared)\n",
    "        \n",
    "       # \"\"\" Add both terms to get the cost \"\"\"\n",
    "        \n",
    "        cost = traditional_cost + weight_decay\n",
    "        \n",
    "       # \"\"\" Compute and unroll 'theta' gradient \"\"\"\n",
    "        \n",
    "        theta_grad = -numpy.dot(ground_truth - probabilities, numpy.transpose(input))\n",
    "        theta_grad = theta_grad / input.shape[1] + self.lamda * theta\n",
    "        theta_grad = numpy.array(theta_grad)\n",
    "        theta_grad = theta_grad.flatten()\n",
    "        \n",
    "        return [cost, theta_grad]\n",
    "    \n",
    "    #######################################################################################\n",
    "  #  \"\"\" Returns predicted classes for a set of inputs \"\"\"\n",
    "            \n",
    "    def softmaxPredict(self, theta, input):\n",
    "    \n",
    "      #  \"\"\" Reshape 'theta' for ease of computation \"\"\"\n",
    "    \n",
    "        theta = theta.reshape(self.num_classes, self.input_size)\n",
    "        \n",
    "    #    \"\"\" Compute the class probabilities for each example \"\"\"\n",
    "        \n",
    "        theta_x       = numpy.dot(theta, input)\n",
    "        hypothesis    = numpy.exp(theta_x)      \n",
    "        probabilities = hypothesis / numpy.sum(hypothesis, axis = 0)\n",
    "        \n",
    "    #    \"\"\" Give the predictions based on probability values \"\"\"\n",
    "        \n",
    "        predictions = numpy.zeros((input.shape[1], 1))\n",
    "        predictions[:, 0] = numpy.argmax(probabilities, axis = 0)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "###########################################################################################\n",
    "#\"\"\" Loads the training images and labels \"\"\"\n",
    "    \n",
    "def loadTrainingDataset():\n",
    "\n",
    "   # \"\"\" Loads the images and labels as numpy arrays\n",
    "    #    The dataset is originally read as a dictionary \"\"\"\n",
    "\n",
    "    train_data   = scipy.io.loadmat('stlTrainSubset.mat')\n",
    "    train_images = numpy.array(train_data['trainImages'])\n",
    "    train_labels = numpy.array(train_data['trainLabels'])\n",
    "    \n",
    "    return [train_images, train_labels]\n",
    "    \n",
    "###########################################################################################\n",
    "#\"\"\" Loads the test images and labels \"\"\"\n",
    "    \n",
    "def loadTestDataset():\n",
    "\n",
    "   # \"\"\" Loads the images and labels as numpy arrays\n",
    "      #  The dataset is originally read as a dictionary \"\"\"\n",
    "\n",
    "    test_data   = scipy.io.loadmat('stlTestSubset.mat')\n",
    "    test_images = numpy.array(test_data['testImages'])\n",
    "    test_labels = numpy.array(test_data['testLabels'])\n",
    "    \n",
    "    return [test_images, test_labels]\n",
    "\n",
    "###########################################################################################\n",
    "#\"\"\" Visualizes the obtained optimal W1 values as images \"\"\"\n",
    "\n",
    "def visualizeW1(opt_W1, vis_patch_side, hid_patch_side):\n",
    "\n",
    "   # \"\"\" Add the weights as a matrix of images \"\"\"\n",
    "    \n",
    "    figure, axes = matplotlib.pyplot.subplots(nrows = hid_patch_side,\n",
    "                                              ncols = hid_patch_side)\n",
    "   #\"\"\" Rescale the values from [-1, 1] to [0, 1] \"\"\"\n",
    "    \n",
    "    opt_W1 = (opt_W1 + 1) / 2\n",
    "    \n",
    "    #\"\"\" Define useful values \"\"\"\n",
    "    \n",
    "    index  = 0\n",
    "    limit0 = 0\n",
    "    limit1 = limit0 + vis_patch_side * vis_patch_side\n",
    "    limit2 = limit1 + vis_patch_side * vis_patch_side\n",
    "    limit3 = limit2 + vis_patch_side * vis_patch_side\n",
    "                                              \n",
    "    for axis in axes.flat:\n",
    "    \n",
    "       # \"\"\" Initialize image as array of zeros \"\"\"\n",
    "    \n",
    "        img = numpy.zeros((vis_patch_side, vis_patch_side, 3))\n",
    "        \n",
    "      #  \"\"\" Divide the rows of parameter values into image channels \"\"\"\n",
    "        \n",
    "        img[:, :, 0] = opt_W1[index, limit0 : limit1].reshape(vis_patch_side, vis_patch_side)\n",
    "        img[:, :, 1] = opt_W1[index, limit1 : limit2].reshape(vis_patch_side, vis_patch_side)\n",
    "        img[:, :, 2] = opt_W1[index, limit2 : limit3].reshape(vis_patch_side, vis_patch_side)\n",
    "        \n",
    "       # \"\"\" Plot the image on the figure \"\"\"\n",
    "        \n",
    "        image = axis.imshow(img, interpolation = 'nearest')\n",
    "        axis.set_frame_on(False)\n",
    "        axis.set_axis_off()\n",
    "        index += 1\n",
    "        \n",
    "   # \"\"\" Show the obtained plot \"\"\"  \n",
    "        \n",
    "    matplotlib.pyplot.show()\n",
    "    \n",
    "###########################################################################################\n",
    "#\"\"\" Returns pooled features for the provided data from a trained network \"\"\"\n",
    "\n",
    "def getPooledFeatures(network, images, num_features, res_dim, step_size):\n",
    "    \n",
    "    num_images = images.shape[3]\n",
    "    \n",
    "   # \"\"\" Initialize pooled features as array of zeros \"\"\"\n",
    "    \n",
    "    pooled_features_data = numpy.zeros((num_features, num_images, res_dim, res_dim))\n",
    "    \n",
    "    for step in range(num_images / step_size):\n",
    "    \n",
    "    #    \"\"\" Limits to access batch of images \"\"\"\n",
    "        \n",
    "        limit0 = step_size * step\n",
    "        limit1 = step_size * (step+1)\n",
    "        \n",
    "        image_batch = images[:, :, :, limit0 : limit1]\n",
    "        \n",
    "       # \"\"\" Calculate pooled features for the image batch \"\"\"\n",
    "    \n",
    "        convolved_features = network.convolve(image_batch, num_features)\n",
    "        pooled_features    = network.pool(convolved_features)\n",
    "        \n",
    "        pooled_features_data[:, limit0 : limit1, :, :] = pooled_features\n",
    "        \n",
    "       # \"\"\" Avoid memory overflow \"\"\"\n",
    "        \n",
    "        del(image_batch)\n",
    "        del(convolved_features)\n",
    "        del(pooled_features)\n",
    "    \n",
    "   # \"\"\" Reshape data for training / testing \"\"\"\n",
    "    \n",
    "    input_size = pooled_features_data.size / num_images\n",
    "    pooled_features_data = numpy.transpose(pooled_features_data, (0, 2, 3, 1))\n",
    "    pooled_features_data = pooled_features_data.reshape(input_size, num_images)\n",
    "    \n",
    "    return pooled_features_data\n",
    "    \n",
    "###########################################################################################\n",
    "#\"\"\" Loads data, trains the Convolutional Neural Network model and predicts classes for test data \"\"\"\n",
    "\n",
    "def executeConvolutionalNeuralNetwork():\n",
    "\n",
    "   # \"\"\" Initialize parameters for the Convolutional Neural Network model \"\"\"\n",
    "\n",
    "    image_dim       = 64     # dimension of the input images\n",
    "    image_channels  = 3      # number of channels in the image patches\n",
    "    vis_patch_side  = 8      # side length of sampled image patches\n",
    "    hid_patch_side  = 20     # side length of representative image patches\n",
    "    pool_dim        = 19     # dimension of patches taken while pooling\n",
    "    \n",
    "    visible_size = vis_patch_side * vis_patch_side * image_channels # number of input units\n",
    "    hidden_size  = hid_patch_side * hid_patch_side                  # number of hidden units\n",
    "    res_dim      = (image_dim - vis_patch_side + 1) / pool_dim      # dimension of pooled features\n",
    "\n",
    "    #\"\"\" Load parameters learned in the SparseAutoencoderLinear exercise \"\"\"\n",
    "\n",
    "    opt_param  = numpy.load('opt_param.npy')\n",
    "    zca_white  = numpy.load('zca_white.npy')\n",
    "    mean_patch = numpy.load('mean_patch.npy')\n",
    "    \n",
    "   # \"\"\" Limits to access 'W1' and 'b1' \"\"\"\n",
    "    \n",
    "    limit0 = 0\n",
    "    limit1 = hidden_size * visible_size\n",
    "    limit2 = 2 * hidden_size * visible_size\n",
    "    limit3 = 2 * hidden_size * visible_size + hidden_size\n",
    "    \n",
    "   # \"\"\" Extract 'W1' and 'b1' from the learned parameters \"\"\"\n",
    "    \n",
    "    opt_W1 = opt_param[limit0 : limit1].reshape(hidden_size, visible_size)\n",
    "    opt_b1 = opt_param[limit2 : limit3].reshape(hidden_size, 1) \n",
    "    \n",
    "   # \"\"\" Visualize the learned optimal W1 weights \"\"\"\n",
    "    \n",
    "    visualizeW1(numpy.dot(opt_W1, zca_white), vis_patch_side, hid_patch_side)\n",
    "    \n",
    "    #\"\"\" Initialize Convolutional Neural Network model \"\"\"\n",
    "    \n",
    "    network = ConvolutionalNeuralNetwork(opt_W1, opt_b1, zca_white, mean_patch, vis_patch_side, pool_dim)\n",
    "    \n",
    "   # \"\"\" Step size for the pooling process\n",
    "        #Pooling done iteratively to avoid memory overflow \"\"\"\n",
    "    \n",
    "    step_size = 50\n",
    "    \n",
    "   # \"\"\" Load training and test data\n",
    "       # Labels are mapped from [1, 2, 3, 4] to [0, 1, 2, 3] \"\"\"\n",
    "    \n",
    "    train_images, train_labels = loadTrainingDataset()\n",
    "    test_images, test_labels   = loadTestDataset()\n",
    "    train_labels = train_labels - 1\n",
    "    test_labels  = test_labels - 1\n",
    "    \n",
    "   # \"\"\" Get pooled features for training and test data \"\"\"\n",
    "    \n",
    "    softmax_train_data = getPooledFeatures(network, train_images, hidden_size, res_dim, step_size)\n",
    "    softmax_test_data  = getPooledFeatures(network, test_images, hidden_size, res_dim, step_size)\n",
    "    \n",
    "   # \"\"\" Initialize parameters of the Regressor \"\"\"\n",
    "    \n",
    "    input_size     = hidden_size * res_dim * res_dim  # input vector size\n",
    "    num_classes    = 4                                # number of classes\n",
    "    lamda          = 0.0001                           # weight decay parameter\n",
    "    max_iterations = 200                              # number of optimization iterations\n",
    "    \n",
    "   # \"\"\" Initialize Softmax Regressor with the above parameters \"\"\"\n",
    "    \n",
    "    regressor = SoftmaxRegression(input_size, num_classes, lamda)\n",
    "    \n",
    "   # \"\"\" Run the L-BFGS algorithm to get the optimal parameter values \"\"\"\n",
    "    \n",
    "    opt_solution  = scipy.optimize.minimize(regressor.softmaxCost, regressor.theta, \n",
    "                                            args = (softmax_train_data, train_labels,), method = 'L-BFGS-B', \n",
    "                                            jac = True, options = {'maxiter': max_iterations})                                        \n",
    "    opt_theta     = opt_solution.x\n",
    "    \n",
    "   # \"\"\" Obtain predictions from the trained model \"\"\"\n",
    "    \n",
    "    predictions = regressor.softmaxPredict(opt_theta, softmax_test_data)\n",
    "    \n",
    "  #  \"\"\" Print accuracy of the trained model \"\"\"\n",
    "    \n",
    "    correct = test_labels[:, 0] == predictions[:, 0]\n",
    "    print(\"\"\"Accuracy :\"\"\"), numpy.mean(correct)\n",
    "\n",
    "executeConvolutionalNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_img_train,y_label_train),(x_img_test,y_label_test)=img_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 128, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "shear_range = 0.2,\n",
    "zoom_range = 0.2,\n",
    "horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('training_set',\n",
    "target_size = (64, 64),\n",
    "batch_size = 32,\n",
    "class_mode = 'binary')\n",
    "test_set = test_datagen.flow_from_directory('test_set',\n",
    "target_size = (64, 64),\n",
    "batch_size = 32,\n",
    "class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit_generator(training_set,\n",
    "steps_per_epoch = 8000,\n",
    "epochs = 25,\n",
    "validation_data = test_set,\n",
    "validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())\n",
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "shear_range = 0.2,\n",
    "zoom_range = 0.2,\n",
    "horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('home/kuas307/pywork/dataset/training_set',\n",
    "target_size = (64, 64),\n",
    "batch_size = 32,\n",
    "class_mode = 'binary')\n",
    "test_set = test_datagen.flow_from_directory('home/kuas307/pywork/dataset/training_set',\n",
    "target_size = (64, 64),\n",
    "batch_size = 32,\n",
    "class_mode = 'binary')\n",
    "classifier.fit_generator(training_set,\n",
    "steps_per_epoch = 8000,\n",
    "epochs = 25,\n",
    "validation_data = test_set,\n",
    "validation_steps = 2000)\n",
    "# Part 3 - Making new predictions\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = classifier.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
